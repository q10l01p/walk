# 目的
确保，当机器人的实际行为与预设的行为向量 $b_t$ 相匹配时，能够获得额外的奖励，同时避免与主任务奖励发生冲突。

举个例子， 如果我们想要通过奖励机制来控制机器人的站姿宽度，一个直观的想法可能是奖励机器 人保持左右两脚之间的一个固定距离。
但是，这种方法在机器人需要快速转弯，脚部需要进行相对侧向移动的任务中，可能会不合理地惩罚机器人。

# Raibert启发奖励



```python
# 计算机器人的每个脚相对于机器人基座（base）的位置。
cur_footsteps_translated = self.env.foot_positions - self.env.base_pos.unsqueeze(1)
```
> `self.env.foot_positions` 是一个张量，其中包含了机器人每个脚的位置。 
>> 形状是 `(num_envs, num_feet, 3)`，其中 `num_envs` 是环境的数量，`num_feet` 是脚的数量，`3` 是三维空间的坐标（x, y, z）。
> 
> `self.env.base_pos` 是一个张量，其中包含了机器人基座的位置。
>> 形状是 `(num_envs, 3)`，其中 `num_envs` 是环境的数量，`3` 是三维空间的坐标（x, y, z）。
> 
> `self.env.base_pos.unsqueeze(1)` 是将 `self.env.base_pos` 张量的形状从 `(num_envs, 3)` 变为 `(num_envs, 1, 3)`。
>> 为了能够与 `self.env.foot_positions` 张量进行广播（broadcasting）操作，即对每个脚的位置都减去相同的基座位置。 
> 
>`cur_footsteps_translated = self.env.foot_positions - self.env.base_pos.unsqueeze(1)` 这行代码的结果是一个新的张量 `cur_footsteps_translated`
>> 形状也是 `(num_envs, num_feet, 3)`，其中每个元素表示了对应脚相对于机器人基座的位置。

```python
# 初始化脚步在身体坐标系中的位置矩阵
footsteps_in_body_frame = torch.zeros(self.env.num_envs, 4, 3, device=self.env.device)
```

```python
# 遍历四个脚步，将机器人的每个脚的位置从全局坐标系转换到机器人基座的局部坐标系
for i in range(4):
    footsteps_in_body_frame[:, i, :] = quat_apply_yaw(quat_conjugate(self.env.base_quat),
                                                      cur_footsteps_translated[:, i, :])
```
> 首先，它遍历机器人的四个脚（`for i in range(4):`）。对于每个脚，它执行以下操作：
> 1. `quat_conjugate(self.env.base_quat)`：计算机器人基座的四元数旋转的共轭。
>> 四元数描述了基座的方向，其共轭用于将全局坐标系下的向量旋转到基座的局部坐标系。
> 2. `cur_footsteps_translated[:, i, :]`：获取当前脚的位置
>> 这个位置是在全局坐标系下，已经减去了基座的位置。
> 3. `quat_apply_yaw(quat_conjugate(self.env.base_quat), cur_footsteps_translated[:, i, :])`：将基座的四元数共轭应用于当前脚的位置，实现了坐标系的转换。
>> 这里使用的`quat_apply_yaw`函数只考虑了偏航角（即绕Z轴的旋转），忽略了俯仰和滚动，这是因为在许多机器人应用中，只关心偏航角。 
> 4. 最后，将转换后的脚的位置存储在`footsteps_in_body_frame`张量中对应的位置。
>> 这个张量的形状是`(num_envs, 4, 3)`，其中`num_envs`是环境的数量，`4`是脚的数量，`3`是三维空间的坐标（x, y, z）。